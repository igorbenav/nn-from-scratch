{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural Network From Scratch with NumPy</h1>\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>0. How Machines Learn</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When we talk about learning, we're actually talking about the process whereby one acquires new knowledge and habilities or improves on their existing skills. Usually one learns by studying or by experience, which means one <b>learns from data</b>. How can we translate this process in such a way that a machine can go through it? Humans learn naturally, it may not be a trivial task to define how to do so in a mathematical sense. Think about it for a bit.</p>\n",
    "\n",
    "<p>We'll loosely say a machine learns a task when it gets better at it by identifying patterns from data. This process involves generalization, so the machine must perform better at the task when shown new examples.</p>\n",
    "\n",
    "<p>Let's say we want a machine to learn to identify whether a picture is a dog or a cat, by this definition, after receiving and analyzing some data $D1$, the success rate of identification on never seen data $D2$ must improve.</p>\n",
    "\n",
    "<p>Now we'll put this into mathematical terms.</p>\n",
    "<p>An algorithm learns the function $f: X \\mapsto Y$ from some data $D = \\{...\\}$ by using the data $D$ to pick a function $g: X \\mapsto Y$ such that $g \\approx f$.</p>\n",
    "\n",
    "<p>Let's go back to the dog or cat identifier, in this case, $f: X \\mapsto Y$ is a functions that receives $x$, a picture belonging to the set of all possible pictures $X$, and this functions returns $y$, a label belonging to the set of labels $Y = \\{dog, cat\\}$.</p>\n",
    "\n",
    "<p align=\"center\"><img src=\"images/dogcatclassifier.jpg\" align=\"center\"></p>\n",
    "<p align=\"center\"><i><a href=\"https://nithanaroy.medium.com/3-ways-to-design-affective-classes-in-ml-classification-algorithms-57a302e5397b\">Source: 3 ways to design effective classes in ML Classification Algorithms</a></i></p>\n",
    "    \n",
    "<p>So, using the data $D = \\{x1, x2, x3 ... xN\\}$, composed by pictures of dogs and cats, our algorithm must pick $g: X \\mapsto Y$, another function that also receives pictures and outputs a lable in $\\{dog, cat\\}$ such that our function $g$ approximates $f$, that is, it classifies cats and dogs in a picture within a defined margin of error.</p>\n",
    "\n",
    "<h3>Supervised and Unsupervised Learning</h3>\n",
    "\n",
    "<p>Now let's go back to our learning definition: an algorithm learns the function $f: X \\mapsto Y$ from some data $D = \\{...\\}$ by using the data $D$ to pick a function $g: X \\mapsto Y$ such that $g \\approx f$. Considering the example $D = \\{x1, x2, x3 ... xN\\}$ where $x_i, i \\in \\{1, 2, 3, ..., N\\}$, is a picture of a cat or a picture of a dog, we're actually performing a specific type of learning called <b>Unsupervised Learning</b>.</p>\n",
    "\n",
    "<p><b>Unsupervised Learning</b> algorithms learn properties of the data by identifying the propability distribution or relevant features that distinguish or cluster the examples in certain classes. Notice that the machine is the one who is picking the characteristics used to classify the data.</p>\n",
    "\n",
    "<p><b>Supervised Learning</b>, on the other hand, learns by experiencing datasets in which each example contains not only a feature, but also an associated <b>label</b> or <b>target</b>. So let's, again, go back to our learning definition and define a dataset that would allow us to perform supervised learning.</p>\n",
    "\n",
    "<p>An algorithm learns the function $f: X \\mapsto Y$ from some data $D = \\{...\\}$ by using the data $D$ to pick a function $g: X \\mapsto Y$ such that $g \\approx f$. So, to perform supervised learning, our dataset must look like this: $D = \\{(x1, y1), (x2, y2), ..., (xN, yN)\\}$ where $x_i, i \\in \\{1, 2, 3, ..., N\\}$, is a picture of a cat or a pitcure of a dog, and $y_i, i \\in \\{1, 2, 3, ..., N\\}$, is an associated label $: \\text{label} \\in \\{dog, cat\\}$</p>.\n",
    "\n",
    "<p align=\"center\"><img src=\"images/supervisedvsunsupervised.png\" align=\"center\"></p>\n",
    "<p align=\"center\"><i><a href=\"https://hal.archives-ouvertes.fr/tel-01881069/document\">Source: Learning a Multiview Weighted Majority Vote Classifier: Using PAC-Bayesian Theory and Boosting</a></i></p>\n",
    "\n",
    "in a simpler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The images illustrate well the distintion. The supervised learning version knows the label for thes examples, so it creates a model that learns how to classify the data it has, and expects that it generalizes well for new data.</p>\n",
    "\n",
    "<p>The unsupervised learning version, on the other hand, does not know the labels for the data, so it clusters data with similar characteristics which the machine decided to be relevant.</p>\n",
    "\n",
    "<h3>I Want More, What's Next?</h3>\n",
    "\n",
    "<p>We are going to focus on <b>supervised learning</b> throughout the rest of this text. The Learning problem is an extensive area with topics such as types of learning, can we learn, how to learn well and a bunch of other interesting things that we are not going to cover here.</p>\n",
    "\n",
    "<p>The content presented so far might be just the absolute minimum so that we can go to the next topic, Neural Networks. Our intent is to use these structures called neural networks to perform machine learning. More of these basics and theoretical knowledge may be teached from neural networks perspectives.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. What is a Neural Network</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's first review what a neural network actually is. A neural network is a collection of connected nodes, with each of these nodes - or neurons - receiving an input and returning an output to the next nodes.</p>\n",
    "<p>A neural network has at least 3 layers, the input layer, which contains the data to be processed; the hidden layer, which performs the main processing; and the output layer, which is the one that actually returns the output we want. Of course, we could have as many hidden layers as we want. Usually, we do not count the input layer when measuring the side of a neural network, so a neural network with one input layer, one hidden layer, and one output layer is classified as a 2 layer neural network.</p>\n",
    "<p>Let's start by a very simple type of neural network, a feedforward multilayer perceptron. Feedforward means there are no cicles or loops in the network, and multilayer perceptron means each of the network's nodes is a perceptron.</p>\n",
    "<p>A more graphical representation below, more on it later.</p>\n",
    "\n",
    "<p align=\"center\"><img src=\"images/neuralnetwork.png\" align=\"center\"></p><br />\n",
    "<p align=\"center\"><i><a href=\"https://www.astroml.org/book_figures/chapter9/fig_neural_network.html\">Image Source: astroML</a></i></p>\n",
    "\n",
    "<p>Our endgoal is to use these networks as an universal function approximator, that is, learning to approximate functions by using neural networks. That's what's called Deep Learning.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. The Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Let's start by breaking this down, and defining a perceptron, or each node of the hidden layer.</p>\n",
    "\n",
    "<p>A Perceptron is a binary classifier, that is, it receives input data, performs some computations, and finally returns a 1 or a 0.</p>\n",
    "\n",
    "<p align=\"center\"><img src=\"images/perceptron.png\" align=\"center\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So we're basically taking an input vector, $x$, element-wise multiplying it by a weight vector $w$, then we sum all of these values. We'll also sum a bias $b$ to this result, and the result of these operations is what we'll cal $y$. Notice this element-wise multiplying two vectors and then summing the result is the same as computing the dot product of the vectors.</p> \n",
    "<p>To summarize this process in an equation:</p>\n",
    "<p>$$y = w^T \\cdot x + b$$</p>\n",
    "<p>Or, more explicitly:</p>\n",
    "<p>$$y = \\sum_{n=1} w_{i}x_{i} + b$$</p>\n",
    "<p>where $w$ and $x$ are vectors of the same length and $b$ is a real number.</p>\n",
    "\n",
    "<p>Now, let's map this y, which has any real number as it's codomain, into $[0, 1]$. To accomplish this, we'll use a function $\\sigma$ whose domain is a real number, and codomain is $[0, 1]$, that is, $\\sigma:\\mathbb{R} \\mapsto [0, 1]$.</p>\n",
    "<p>We'll call this $\\sigma$ function the activation function, more on activation functions later.</p>\n",
    "<p>So, the output of the perceptron, $\\hat{y}$, can be described as:</p>\n",
    "<p>$$\\hat{y} = \\sigma(w^T \\cdot x + b)$$</p>\n",
    "<p>or</p>\n",
    "<p>$$\\hat{y} = \\sigma(\\sum_{n=1} w_{i}x_{i} + b)$$</p>\n",
    "<p>Where $\\sigma$ is the activation function $\\sigma:\\mathbb{R} \\mapsto [0, 1]$, $w$ and $x$ are vectors of same length which represent respectively the weights and the inputs, and $b$ is a real number, the bias.</p>\n",
    "<p>Finally, we must set a <b>treshold</b>, that is, a number that lets us map the output into actually two classes, a 1 or a 0. For now, let's just use 0.5. So, \n",
    "\n",
    "$$\\text{if} \\:\\: \\sigma(w^T \\cdot x + b) \\leq 0.5, \\:\\: then \\:\\: \\hat{y} = 0 $$\n",
    "$$\\text{if} \\:\\: \\sigma(w^T \\cdot x + b) > 0.5, \\:\\: then \\:\\: \\hat{y} = 1$$\n",
    "\n",
    "<p>or, </p>\n",
    "$$\\begin{equation*}\n",
    "\\hat{y}=\\begin{cases}\n",
    "          0 \\quad &\\text{if} \\:\\: \\, \\sigma(w^T \\cdot x + b) \\leq 0.5 \\\\\n",
    "          1 \\quad &\\text{if} \\:\\: \\, \\sigma(w^T \\cdot x + b) > 0.5 \\\\\n",
    "     \\end{cases}\n",
    "\\end{equation*}$$\n",
    "<p>Now let's define a perceptron in python:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonical importing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following as our activation function, more on activation functions later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + (np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x, w, b=0, sigma=sigmoid):\n",
    "    '''\n",
    "    this function receives x and w, numpy vectors of same length and optionally b, a real number,\n",
    "    and sigma, an activation function\n",
    "    \n",
    "    it returns a float which represents the probability that the vector is of a certain class, 1 \n",
    "    '''\n",
    "    return sigma(np.dot(w, x) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before making it binary: 0.8937850083248244\n",
      "After: 1\n"
     ]
    }
   ],
   "source": [
    "my_x = np.array([1, 3, 2])\n",
    "my_w = np.array([-0.5, 0.01, 0.8])\n",
    "my_b = 1\n",
    "\n",
    "y_hat = perceptron(my_x, my_w, my_b)\n",
    "print('Before making it binary: {}'.format(y_hat))\n",
    "\n",
    "if y_hat <= 0.5:\n",
    "    y_hat = 0\n",
    "else:\n",
    "    y_hat = 1\n",
    "    \n",
    "print('After: {}'.format(y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
